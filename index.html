
<!DOCTYPE html>
<html>
<head>
    <title>Debug Dual Synth</title>
    <style>
        body { margin: 0; background: #111; color: #fff; font-family: monospace; overflow: hidden; }
        #log { position: absolute; bottom: 10px; left: 10px; background: rgba(0,0,0,0.8); padding: 10px; font-size: 10px; max-height: 100px; overflow-y: auto; width: 300px; border: 1px solid #444; z-index: 100; }
        canvas { width: 100vw; height: 100vh; object-fit: contain; }
        #startBtn { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); padding: 25px; z-index: 20; background: cyan; font-weight: bold; cursor: pointer; }
        video { display: none; }
    </style>
</head>
<body>

    <div id="log">--- System Logs ---<br></div>
    <button id="startBtn">1. INITIALIZE SYSTEM</button>
    <video id="webcam" playsinline></video>
    <canvas id="output_canvas"></canvas>

    <script type="module">
        import { HandLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

        const logDiv = document.getElementById("log");
        function logger(msg) { logDiv.innerHTML += `> ${msg}<br>`; logDiv.scrollTop = logDiv.scrollHeight; }

        const video = document.getElementById("webcam");
        const canvasElement = document.getElementById("output_canvas");
        const canvasCtx = canvasElement.getContext("2d");
        const startBtn = document.getElementById("startBtn");

        let handLandmarker;
        let audioCtx, oscillator, gainNode;

        async function initAI() {
            try {
                logger("Fetching MediaPipe files...");
                const vision = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm");
                
                logger("Loading Model (this may take a sec)...");
                handLandmarker = await HandLandmarker.createFromOptions(vision, {
                    baseOptions: { 
                        modelAssetPath: "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task",
                        delegate: "CPU" // Switched to CPU for stability
                    },
                    runningMode: "VIDEO",
                    numHands: 2
                });
                logger("AI Ready!");
                startBtn.innerText = "2. START CAMERA & SOUND";
            } catch (err) {
                logger("ERROR: " + err.message);
            }
        }

        initAI();

        startBtn.addEventListener("click", async () => {
            if (!handLandmarker) return logger("Wait for AI to load...");
            
            try {
                logger("Starting Audio...");
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                oscillator = audioCtx.createOscillator();
                gainNode = audioCtx.createGain();
                gainNode.gain.setValueAtTime(0, audioCtx.currentTime);
                oscillator.connect(gainNode);
                gainNode.connect(audioCtx.destination);
                oscillator.start();

                logger("Requesting Camera...");
                const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 480 } });
                video.srcObject = stream;
                await video.play();
                
                startBtn.style.display = "none";
                logger("System Live.");
                requestAnimationFrame(loop);
            } catch (err) {
                logger("START ERROR: " + err.message);
            }
        });

        function loop() {
            if (video.readyState >= 2) {
                const results = handLandmarker.detectForVideo(video, performance.now());
                
                canvasElement.width = video.videoWidth;
                canvasElement.height = video.videoHeight;
                canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
                
                // Draw mirrored video
                canvasCtx.save();
                canvasCtx.translate(canvasElement.width, 0);
                canvasCtx.scale(-1, 1);
                canvasCtx.drawImage(video, 0, 0, canvasElement.width, canvasElement.height);

                let rHand = false;
                let lHand = false;

                if (results.landmarks) {
                    results.landmarks.forEach((pts, i) => {
                        const label = results.handedness[i][0].categoryName;
                        const dist = Math.hypot(pts[4].x - pts[8].x, pts[4].y - pts[8].y);

                        if (label === "Right") {
                            rHand = true;
                            oscillator.frequency.setTargetAtTime(200 + (dist * 2500), audioCtx.currentTime, 0.05);
                        } else {
                            lHand = true;
                            gainNode.gain.setTargetAtTime(Math.min(0.3, dist * 2), audioCtx.currentTime, 0.05);
                        }
                    });
                }

                if (!lHand && audioCtx) gainNode.gain.setTargetAtTime(0, audioCtx.currentTime, 0.1);
                canvasCtx.restore();
            }
            requestAnimationFrame(loop);
        }
    </script>
</body>
</html>
