
<!DOCTYPE html>
<html>
<head>
    <title>Fixed Dual Synth</title>
    <style>
        body { margin: 0; background: #000; color: #0f0; font-family: monospace; overflow: hidden; }
        #ui { position: absolute; top: 10px; left: 10px; z-index: 10; pointer-events: none; background: rgba(0,0,0,0.8); padding: 10px; border: 1px solid #0f0; }
        canvas { width: 100vw; height: 100vh; object-fit: contain; }
        #startBtn { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); padding: 30px; font-size: 20px; z-index: 20; background: #0f0; cursor: pointer; }
        video { display: none; }
    </style>
</head>
<body>

    <div id="ui">
        <div id="status">Status: Loading AI...</div>
        <div id="data">Right: --- | Left: ---</div>
    </div>

    <button id="startBtn">CLICK TO START SENSORS</button>
    <video id="webcam" playsinline></video>
    <canvas id="output_canvas"></canvas>

    <script type="module">
        import { HandLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

        const video = document.getElementById("webcam");
        const canvasElement = document.getElementById("output_canvas");
        const canvasCtx = canvasElement.getContext("2d");
        const startBtn = document.getElementById("startBtn");
        const status = document.getElementById("status");
        const dataBox = document.getElementById("data");

        let handLandmarker;
        let audioCtx, oscillator, gainNode;

        // 1. Load AI Models safely
        async function loadAI() {
            try {
                const vision = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm");
                handLandmarker = await HandLandmarker.createFromOptions(vision, {
                    baseOptions: { 
                        modelAssetPath: "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task",
                        delegate: "GPU" 
                    },
                    runningMode: "VIDEO",
                    numHands: 2
                });
                status.innerText = "Status: AI Ready. Click Start.";
            } catch (e) {
                status.innerText = "Status: AI Load Failed: " + e.message;
            }
        }
        loadAI();

        // 2. Start Audio/Video on Click
        startBtn.addEventListener("click", async () => {
            try {
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                oscillator = audioCtx.createOscillator();
                gainNode = audioCtx.createGain();
                oscillator.type = "sine";
                gainNode.gain.setValueAtTime(0, audioCtx.currentTime);
                oscillator.connect(gainNode);
                gainNode.connect(audioCtx.destination);
                oscillator.start();

                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                video.play();
                startBtn.style.display = "none";
                status.innerText = "Status: Running...";
                requestAnimationFrame(loop);
            } catch (e) {
                alert("Init Error: " + e.message);
            }
        });

        // 3. Robust Detection Loop
        function loop() {
            if (video.readyState === 4) {
                canvasElement.width = video.videoWidth;
                canvasElement.height = video.videoHeight;
                
                const results = handLandmarker.detectForVideo(video, performance.now());
                
                canvasCtx.save();
                canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
                canvasCtx.translate(canvasElement.width, 0);
                canvasCtx.scale(-1, 1);
                canvasCtx.drawImage(video, 0, 0, canvasElement.width, canvasElement.height);

                let activeRight = false;
                let activeLeft = false;

                if (results.landmarks && results.landmarks.length > 0) {
                    results.landmarks.forEach((landmarks, i) => {
                        const side = results.handedness[i][0].categoryName;
                        const t = landmarks[4]; // Thumb
                        const f = landmarks[8]; // Index
                        const dist = Math.hypot(t.x - f.x, t.y - f.y);

                        if (side === "Right") {
                            activeRight = true;
                            const freq = 200 + (dist * 2000);
                            if (!isNaN(freq)) oscillator.frequency.setTargetAtTime(freq, audioCtx.currentTime, 0.05);
                        } else {
                            activeLeft = true;
                            const vol = Math.min(0.5, dist * 2);
                            if (!isNaN(vol)) gainNode.gain.setTargetAtTime(vol, audioCtx.currentTime, 0.05);
                        }

                        // Draw visual line
                        canvasCtx.strokeStyle = side === "Right" ? "#0f0" : "#f0f";
                        canvasCtx.lineWidth = 10;
                        canvasCtx.beginPath();
                        canvasCtx.moveTo(t.x * canvasElement.width, t.y * canvasElement.height);
                        canvasCtx.lineTo(f.x * canvasElement.width, f.y * canvasElement.height);
                        canvasCtx.stroke();
                    });
                }

                // Fade out if hands leave
                if (!activeLeft) gainNode.gain.setTargetAtTime(0, audioCtx.currentTime, 0.1);
                
                dataBox.innerText = `R: ${activeRight ? 'OK' : '--'} | L: ${activeLeft ? 'OK' : '--'}`;
                canvasCtx.restore();
            }
            requestAnimationFrame(loop);
        }
    </script>
</body>
</html>
